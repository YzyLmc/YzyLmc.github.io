---
layout: post
title: On-Going>>  Improving Vision-based Text Generation by Multi-modal Reinforcement Learning
excerpt: A new method to enhance the performance of language generation model for vision-language navigation by using REINFORCE with multi-modal rewards
# theme: simple

tags: [presentation]
category: presentation
---
## Table of Contents


---
## Introduction
### RL in language generation
![MLE_BLEU](/images/bleu.png)

Reinforcement Learning (__RL__) in language generation has been investigated for a long time, particularly because it's supposed to be a solution for exposure bias and different learning targets during training and testing (train with MLE, test with BLEU). Policy gradient methods have been implemented widely, while the efficiency of RL is always questioned. In general, RL method suffers from high-dimensional discrete action space, reward sparsity, and easy to be trapped in local minimum.  

![peak](/images/peak.png)

Due to above reasons, RL in language generation has been controversial. 'On the Weaknesses of Reinforcement Learning
for Neural Machine Translation' ([Choshen et al](https://arxiv.org/abs/1907.01752)) and 'Revisiting the Weaknesses of Reinforcement Learning
for Neural Machine Translation' ([Kiegeland et al](https://arxiv.org/abs/2106.08942)) are two interesting papers to read. Chosen et al argued that RL just modified the probability distribution to be more 'peaked'. The most astonishing point they found is that the improvement over the pretrained model appears even when they replaced the reward function (BLEU score, in their case) by a constant reward. Following Choshen et al, Kiegeland et al conducted further experiments and showed empirically that empirical improvements
over a strong pretrained model vanish when there is little to learn from the new feedback, which means the reason of no obvious improvement is observed from Choshen et al's experiment is that minimizing MLE and maximizing BLEU score are optimizations at same objective.

### GAN in language Generation

![GAN](/images/GAN.png)

Generative Adversarial Model (__GAN__) for language generation as well been studied broadly. The biggest challenge in implementing GAN in language generation model is that the output of the generator (language generation model) is manually 'picked' at each timestep of the decoding phase. In other words, we pick the token according to the probability distribution of the generation model's output instead of using the probability itself as the result, which prevent us from backpropagating the GAN loss through the generator (in this sense, GAN loss is similar to BLEU for they're both non-differential). seqGAN by Yu et al introduced REINFORCE with Monte-Carlo roll-out for backpropagating, and there are also non-RL GANs like CoT and FM-GAN. However, at the time of 2019, a famous paper, ['Language GANs Falling Short' by Caccia et al](https://arxiv.org/abs/1811.02549), came out. It pointed out that GAN model in language generation is flawed by proving MLE models can outperform all existing GAN models when both quality and diversity are taking into account ('Temperature Sweep'). After that, language GANs became less popular.

### Language Generation in VLN

Language generation model in vision-language generation is first introduced by [Fried et al](https://arxiv.org/abs/1806.02724) in the Speaker-Follower model. Following Speaker-Follower, Tan et al proposed EnvDrop model, which improve the generation model by stacking multiple LSTM networks. It has been empirically showed that generation models can substantially improve the 'Speaker' (grounding model) by augmenting instruction-trajectory pairs and then deploying curriculum training on the augmented dataset.

![zhao_et_al](/images/zhao.png)  
However, instructions generated by the language generation model are usually hard to evaluate, especially when the training objective is hard to express in a mathematical form like in our case, where the ultimate goal is to make generated instructions understandable to human. In this sense, [Zhao et al](https://arxiv.org/abs/2101.10504) evaluated different metrics on the topic of evaluating generated instructions. An important finding is traditional language metric such as BLEU, METEOR, and CIDEr are inefficient on evaluating the quality of generated instructions, while SPICE is efficient on system-level evaluation, and BERTScore is efficient on evaluating model-generated instructions, and using VLN agents as evaluation model will carry out promising result on evaluating all instructions. What's more, they also proposed a compatibility neural network for evaluating instructions on all purposes.

---

## Proposed Method

Starting with the pretrained generator, we uses the REINFORCE to backpropagate non-differentiable learning target to the model. The reward function is consisting of two parts: BERTScore and VLN agents. The second reward (Agent) explores the idea of training a generator directly to produce better instructions. We use a pretrained grounding model to take actions in the simulator according to the generated instructions, and return reward signal equal to the distance between the goal location and the agentâ€™s final location.

![RLVLN](/images/RLVLN.png)
